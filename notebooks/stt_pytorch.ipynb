{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJEMjPgeI-rw",
    "outputId": "7491c067-b1be-4505-b3f5-19ba4c00a593"
   },
   "outputs": [],
   "source": [
    "!pip install moshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn nest_asyncio pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA3Haix3IZ8Q"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import time\n",
    "import sentencepiece\n",
    "import sphn\n",
    "import textwrap\n",
    "import torch\n",
    "\n",
    "from moshi.models import loaders, MimiModel, LMModel, LMGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import os\n",
    "import threading\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ngrok authtoken YOUR_NGROK_AUTH_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AK5zBMTI9bw"
   },
   "outputs": [],
   "source": [
    "TRANSCRIPT_BUFFER = \"\"\n",
    "\n",
    "@dataclass\n",
    "class InferenceState:\n",
    "    mimi: MimiModel\n",
    "    text_tokenizer: sentencepiece.SentencePieceProcessor\n",
    "    lm_gen: LMGen\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mimi: MimiModel,\n",
    "        text_tokenizer: sentencepiece.SentencePieceProcessor,\n",
    "        lm: LMModel,\n",
    "        batch_size: int,\n",
    "        device: str | torch.device,\n",
    "    ):\n",
    "        self.mimi = mimi\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.lm_gen = LMGen(lm, temp=0, temp_text=0, use_sampling=False)\n",
    "        self.device = device\n",
    "        self.frame_size = int(self.mimi.sample_rate / self.mimi.frame_rate)\n",
    "        self.batch_size = batch_size\n",
    "        self.mimi.streaming_forever(batch_size)\n",
    "        self.lm_gen.streaming_forever(batch_size)\n",
    "\n",
    "    def run(self, in_pcms: torch.Tensor):\n",
    "        ntokens = 0\n",
    "        first_frame = True\n",
    "        chunks = [\n",
    "            c\n",
    "            for c in in_pcms.split(self.frame_size, dim=2)\n",
    "            if c.shape[-1] == self.frame_size\n",
    "        ]\n",
    "        start_time = time.time()\n",
    "        all_text = []\n",
    "        for chunk in chunks:\n",
    "            codes = self.mimi.encode(chunk)\n",
    "            if first_frame:\n",
    "                # Ensure that the first slice of codes is properly seen by the transformer\n",
    "                # as otherwise the first slice is replaced by the initial tokens.\n",
    "                tokens = self.lm_gen.step(codes)\n",
    "                first_frame = False\n",
    "            tokens = self.lm_gen.step(codes)\n",
    "            if tokens is None:\n",
    "                continue\n",
    "            assert tokens.shape[1] == 1\n",
    "            one_text = tokens[0, 0].cpu()\n",
    "            if one_text.item() not in [0, 3]:\n",
    "                text = self.text_tokenizer.id_to_piece(one_text.item())\n",
    "                text = text.replace(\"‚ñÅ\", \" \")\n",
    "                all_text.append(text)\n",
    "            ntokens += 1\n",
    "        dt = time.time() - start_time\n",
    "        print(\n",
    "            f\"processed {ntokens} steps in {dt:.0f}s, {1000 * dt / ntokens:.2f}ms/step\"\n",
    "        )\n",
    "        return \"\".join(all_text)\n",
    "\n",
    "class AudioChunk(BaseModel):\n",
    "    type: str  # \"audio_chunk\"\n",
    "    pcm: list[float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# Use the en+fr low latency model, an alternative is kyutai/stt-2.6b-en\n",
    "checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\"kyutai/stt-1b-en_fr\")\n",
    "mimi = checkpoint_info.get_mimi(device=device)\n",
    "text_tokenizer = checkpoint_info.get_text_tokenizer()\n",
    "lm = checkpoint_info.get_moshi(device=device)\n",
    "inference_state = InferenceState(mimi, text_tokenizer, lm, batch_size=1, device=device)\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"Colab FastAPI is working!\"}\n",
    "\n",
    "@app.post(\"/stt\")\n",
    "async def receive_audio(request: Request):\n",
    "    global TRANSCRIPT_BUFFER\n",
    "    data = await request.json()\n",
    "    # Convert back to numpy array\n",
    "    audio_np = np.array(data[\"pcm\"], dtype=np.float32)\n",
    "\n",
    "    audio_tensor = torch.from_numpy(audio_np)[None, None, :]\n",
    "    text_chunk = inference_state.run(audio_tensor)\n",
    "    TRANSCRIPT_BUFFER += \" \" + text_chunk\n",
    "\n",
    "    return {\"text\": text_chunk}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import time\n",
    "public_url = ngrok.connect('7680')\n",
    "print(public_url)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "log_capture_buffer = io.StringIO()\n",
    "\n",
    "log_file = \"/content/server.log\"\n",
    "def run():\n",
    "    try:\n",
    "      uvicorn.run(app, host=\"0.0.0.0\", port=7680, log_level=\"debug\")\n",
    "    finally:\n",
    "      sys.stderr = sys.__stderr__\n",
    "\n",
    "\n",
    "server_thread = threading.Thread(target=run, daemon=True)\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_server_logs():\n",
    "    \"\"\"Prints and clears the captured logs.\"\"\"\n",
    "    logs = log_capture_buffer.getvalue()\n",
    "    if logs:\n",
    "        print(\"\\n--- SERVER LOGS/ERRORS ---\")\n",
    "        print(logs, end='')\n",
    "        # Clear the buffer after printing to avoid duplicates\n",
    "        log_capture_buffer.truncate(0)\n",
    "        log_capture_buffer.seek(0)\n",
    "        print(\"--------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  time.sleep(3)\n",
    "  print_server_logs()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
